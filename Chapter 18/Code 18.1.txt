import java.io.*;
importjava.util.*;

importcom.sforce.async.*;
importcom.sforce.soap.partner.PartnerConnection;
importcom.sforce.ws.ConnectionException;
importcom.sforce.ws.ConnectorConfig;


publicclassBulkTest {

publicstaticvoid main(String[] args)
throwsAsyncApiException, ConnectionException, IOException {
BulkTestexample = newBulkTest();
// Replace arguments below with your credentials and test file name
// The first parameter indicates that we are loading Account records
example.runSample("Account", "Enter your Salesforce Org Username here", "Enter Org password immediately followed by Security Token", "Enter path to your sample CSV file (For this module the path is F:\\Soap\\test\\mySampleData.csv)");
    }

/**
     * Creates a Bulk API job and uploads batches for a CSV file.
     */
publicvoidrunSample(String sobjectType, String userName,
              String password, String sampleFileName)
throwsAsyncApiException, ConnectionException, IOException {
BulkConnectionconnection = getBulkConnection(userName, password);
JobInfojob = createJob(sobjectType, connection);
        List<BatchInfo>batchInfoList = createBatchesFromCSVFile(connection, job,
sampleFileName);
closeJob(connection, job.getId());
awaitCompletion(connection, job, batchInfoList);
checkResults(connection, job, batchInfoList);
    }

/**
     * Gets the results of the operation and checks for errors.
     */
privatevoidcheckResults(BulkConnectionconnection, JobInfojob,
              List<BatchInfo>batchInfoList)
throwsAsyncApiException, IOException {
// batchInfoList was populated when batches were created and submitted
for (BatchInfob : batchInfoList) {
CSVReaderrdr =
newCSVReader(connection.getBatchResultStream(job.getId(), b.getId()));
            List<String>resultHeader = rdr.nextRecord();
intresultCols = resultHeader.size();

            List<String>row;
while ((row = rdr.nextRecord()) != null) {
                Map<String, String>resultInfo = newHashMap<String, String>();
for (inti = 0; i<resultCols; i++) {
resultInfo.put(resultHeader.get(i), row.get(i));
                }
booleansuccess = Boolean.valueOf(resultInfo.get("Success"));
booleancreated = Boolean.valueOf(resultInfo.get("Created"));
                String id = resultInfo.get("Id");
                String error = resultInfo.get("Error");
if (success&&created) {
System.out.println("Created row with id " + id);
                } elseif (!success) {
System.out.println("Failed with error: " + error);
                }
            }
        }
    }


privatevoidcloseJob(BulkConnectionconnection, String jobId)
throwsAsyncApiException {
JobInfojob = newJobInfo();
job.setId(jobId);
job.setState(JobStateEnum.Closed);
connection.updateJob(job);
    }



/**
     * Wait for a job to complete by polling the Bulk API.
     * 
     * @param connection
     *            BulkConnection used to check results.
     * @param job
     *            The job awaiting completion.
     * @parambatchInfoList
     *            List of batches for this job.
     * @throwsAsyncApiException
     */
privatevoidawaitCompletion(BulkConnectionconnection, JobInfojob,
          List<BatchInfo>batchInfoList)
throwsAsyncApiException {
longsleepTime = 0L;
        Set<String>incomplete = newHashSet<String>();
for (BatchInfobi : batchInfoList) {
incomplete.add(bi.getId());
        }
while (!incomplete.isEmpty()) {
try {
Thread.sleep(sleepTime);
            } catch (InterruptedExceptione) {}
System.out.println("Awaiting results..." + incomplete.size());
sleepTime = 10000L;
BatchInfo[] statusList =
connection.getBatchInfoList(job.getId()).getBatchInfo();
for (BatchInfob : statusList) {
if (b.getState() == BatchStateEnum.Completed
                  || b.getState() == BatchStateEnum.Failed) {
if (incomplete.remove(b.getId())) {
System.out.println("BATCH STATUS:\n" + b);
                    }
                }
            }
        }
    }



/**
     * Create a new job using the Bulk API.
     * 
     * @paramsobjectType
     *            The object type being loaded, such as "Account"
     * @param connection
     *            BulkConnection used to create the new job.
     * @return The JobInfo for the new job.
     * @throwsAsyncApiException
     */
privateJobInfocreateJob(String sobjectType, BulkConnectionconnection)
throwsAsyncApiException {
JobInfojob = newJobInfo();
job.setObject(sobjectType);
job.setOperation(OperationEnum.insert);
job.setContentType(ContentType.CSV);
job = connection.createJob(job);
System.out.println(job);
returnjob;
    }



/**
     * Create the BulkConnection used to call Bulk API operations.
     */
privateBulkConnectiongetBulkConnection(String userName, String password)
throwsConnectionException, AsyncApiException {
ConnectorConfigpartnerConfig = newConnectorConfig();
partnerConfig.setUsername(userName);
partnerConfig.setPassword(password);
partnerConfig.setAuthEndpoint("https://login.salesforce.com/services/Soap/u/43.0");
// Creating the connection automatically handles login and stores
// the session in partnerConfig
newPartnerConnection(partnerConfig);
// When PartnerConnection is instantiated, a login is implicitly
// executed and, if successful,
// a valid session is stored in the ConnectorConfig instance.
// Use this key to initialize a BulkConnection:
ConnectorConfigconfig = newConnectorConfig();
config.setSessionId(partnerConfig.getSessionId());
// Theendpoint for the Bulk API service is the same as for the normal
// SOAP uri until the /Soap/ part. From here it's '/async/versionNumber'
        String soapEndpoint = partnerConfig.getServiceEndpoint();
        String apiVersion = "43.0";
        String restEndpoint = soapEndpoint.substring(0, soapEndpoint.indexOf("Soap/"))
            + "async/" + apiVersion;
config.setRestEndpoint(restEndpoint);
// This should only be false when doing debugging.
config.setCompression(true);
// Set this to true to see HTTP requests and responses on stdout
config.setTraceMessage(false);
BulkConnectionconnection = newBulkConnection(config);
returnconnection;
    }



/**
     * Create and upload batches using a CSV file.
     * The file into the appropriate size batch files.
     * 
     * @param connection
     *            Connection to use for creating batches
     * @paramjobInfo
     *            Job associated with new batches
     * @paramcsvFileName
     *            The source file for batch data
     */
private List<BatchInfo>createBatchesFromCSVFile(BulkConnectionconnection,
JobInfojobInfo, String csvFileName)
throwsIOException, AsyncApiException {
        List<BatchInfo>batchInfos = newArrayList<BatchInfo>();
BufferedReaderrdr = newBufferedReader(
newInputStreamReader(newFileInputStream(csvFileName))
        );
// read the CSV header row
byte[] headerBytes = (rdr.readLine() + "\n").getBytes("UTF-8");
intheaderBytesLength = headerBytes.length;
        File tmpFile = File.createTempFile("bulkAPIInsert", ".csv");

// Split the CSV file into multiple batches
try {
FileOutputStreamtmpOut = newFileOutputStream(tmpFile);
intmaxBytesPerBatch = 10000000; // 10 million bytes per batch
intmaxRowsPerBatch = 10000; // 10 thousand rows per batch
intcurrentBytes = 0;
intcurrentLines = 0;
            String nextLine;
while ((nextLine = rdr.readLine()) != null) {
byte[] bytes = (nextLine + "\n").getBytes("UTF-8");
// Create a new batch when our batch size limit is reached
if (currentBytes + bytes.length>maxBytesPerBatch
                  || currentLines>maxRowsPerBatch) {
createBatch(tmpOut, tmpFile, batchInfos, connection, jobInfo);
currentBytes = 0;
currentLines = 0;
                }
if (currentBytes == 0) {
tmpOut = newFileOutputStream(tmpFile);
tmpOut.write(headerBytes);
currentBytes = headerBytesLength;
currentLines = 1;
                }
tmpOut.write(bytes);
currentBytes += bytes.length;
currentLines++;
            }
// Finished processing all rows
// Create a final batch for any remaining data
if (currentLines> 1) {
createBatch(tmpOut, tmpFile, batchInfos, connection, jobInfo);
            }
        } finally {
tmpFile.delete();
        }
returnbatchInfos;
    }

/**
     * Create a batch by uploading the contents of the file.
     * This closes the output stream.
     * 
     * @paramtmpOut
     *            The output stream used to write the CSV data for a single batch.
     * @paramtmpFile
     *            The file associated with the above stream.
     * @parambatchInfos
     *            The batch info for the newly created batch is added to this list.
     * @param connection
     *            The BulkConnection used to create the new batch.
     * @paramjobInfo
     *            The JobInfo associated with the new batch.
     */
privatevoidcreateBatch(FileOutputStreamtmpOut, File tmpFile,
      List<BatchInfo>batchInfos, BulkConnectionconnection, JobInfojobInfo)
throwsIOException, AsyncApiException {
tmpOut.flush();
tmpOut.close();
FileInputStreamtmpInputStream = newFileInputStream(tmpFile);
try {
BatchInfobatchInfo =
connection.createBatchFromStream(jobInfo, tmpInputStream);
System.out.println(batchInfo);
batchInfos.add(batchInfo);

        } finally {
tmpInputStream.close();
        }
    }


}